### 下载优美图库图片

[网址](https://www.umei.cc/)

我们选择下载唯美壁纸的前20页图片，也就是600张。先获取这20个页面的`url`。

```python
def get_urls():
    urls = []
    url = "https://www.umei.cc/bizhitupian/weimeibizhi/"
    # 只要把这个地方改一改就能下载其他类型的图片了
    urls.append(url)
    for i in range(2, 21):# 选择下载多少页
        urls.append(url + f"index_{i}.htm")
    return urls
```

然后从这20个主页面获取页面上30张图片的`url`，加到总的`child_urls`列表。

```python
def get_child_url(url):
    resp = requests.get(url)
    resp.encoding = 'utf-8'
    main_page = BeautifulSoup(resp.text, 'html.parser')
    alist = main_page.find('div', class_="TypeList").find_all('a')
    for a in alist:
        href = a.get('href')
        child_urls.append("https://www.umei.cc" + href)
    resp.close()
```

下载一个`child_url`上面的图片。首先获取图片链接，然后下载到本地。这里加了一个很粗浅的异常处理，避免以为一个图片下载失败让整个程序崩溃。

```python
def download_url(url):
    try:
        resp = requests.get(url)
        resp.encoding = 'utf-8'
        page = BeautifulSoup(resp.text, 'html.parser')
        picture = page.find('div', class_="ImageBody").find('img')
        img_url = picture.get('src')
        # print(img_url)
        img_resp = requests.get(img_url)
        img_name = img_url.split("/")[-1]
        with open("微信公众号/" + img_name, mode="wb") as f:
            f.write(img_resp.content)
        print(url + 'ok')
        img_resp.close()
        child_resp.close()
    except FileNotFoundError:
        print("FileNotFoundError")
    else:
        print(url)
```

但是呢，一次下一张图片，600张图片何时是个头，所以我们搞100个线程，一次下100张，咻咻咻就下完了。

```python
    with ThreadPoolExecutor(100) as t:
        for child_url in child_urls:
            t.submit(download_url, child_url)
```

好了，完整代码如下：

```python
import requests
from bs4 import BeautifulSoup
import time
from concurrent.futures import ThreadPoolExecutor

child_urls = []


def get_urls():
    urls = []
    url = "https://www.umei.cc/bizhitupian/weimeibizhi/"
    urls.append(url)
    for i in range(2, 21):
        urls.append(url + f"index_{i}.htm")
    return urls


def get_child_url(url):
    resp = requests.get(url)
    resp.encoding = 'utf-8'
    main_page = BeautifulSoup(resp.text, 'html.parser')
    alist = main_page.find('div', class_="TypeList").find_all('a')
    for a in alist:
        href = a.get('href')
        child_urls.append("https://www.umei.cc" + href)
    resp.close()


def download_url(url):
    try:
        resp = requests.get(url)
        resp.encoding = 'utf-8'
        page = BeautifulSoup(resp.text, 'html.parser')
        picture = page.find('div', class_="ImageBody").find('img')
        img_url = picture.get('src')
        # print(img_url)
        img_resp = requests.get(img_url)
        img_name = img_url.split("/")[-1]
        with open("微信公众号/" + img_name, mode="wb") as f:
            f.write(img_resp.content)
        print(url + 'ok')
        img_resp.close()
        child_resp.close()
    except FileNotFoundError:
        print("FileNotFoundError")
    else:
        print(url)


if __name__ == '__main__':
    t1 = time.time()
    urls = get_urls()
    for url in urls:
        get_child_url(url)
    with ThreadPoolExecutor(100) as t:
        for child_url in child_urls:
            t.submit(download_url, child_url)
    t2 = time.time()
    print(t2 - t1)
```

好了，结果展示：

![image-20211024181322182](C:\Users\pikachu\AppData\Roaming\Typora\typora-user-images\image-20211024181322182.png)

